{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Merger Components\n",
    "\n",
    "The first part of the data cleaning tool is the merger. It is intended to help discover and merge data from multiple \n",
    "sources. For data sources, we mean table-based data, primarily sitting in Excel spreadsheets, but tool is designed so that it should be able to deal with other table-based formats, such as CVS.\n",
    "\n",
    "We will be loading sources from our parent directory, so we need to tweak the package path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Tables\n",
    "\n",
    "The Front Door Data Collaboration merge tool uses a concept of Data Sources with defined Columns. Data Sources and Columns have fixed names, and a few additional parameters. They can both have a set of matching rules so that any scanned sources can be matched with a defined Data Source. For example, say an scanned Excel workbook has a worksheet named \"list_1\" - this could be matched with a Data Source called \"List 1\" based on the configuration. \n",
    "\n",
    "Default matchers are defined when loading the configuration file, but additional options can be added in the configuration. \n",
    "\n",
    "Let's start by loading the configuration for Annex A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from fddc.annex_a.merger import configuration\n",
    "\n",
    "data_sources = configuration.parse_datasources(\"../config/annex-a-merge.yml\")\n",
    "asdict(data_sources[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering data\n",
    "\n",
    "The first part of the process is file discovery. This is a very simple glob based pattern to help discover target files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from fddc.annex_a.merger import file_scanner\n",
    "\n",
    "files = file_scanner.find_input_files(\"../examples/*.xls*\")\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting a sort key\n",
    "\n",
    "Often we want to make sure files are sorted in the right order based on, quite commonly, the year. We can see that examples above sort lexically in the 'wrong' order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = file_scanner.find_input_files(\n",
    "    file_scanner.ScanSource(\"../examples/*.xls*\", sort_keys=[r'/.*?-(?P<cat>\\w)-(?P<year>\\d+).*/\\g<year>-\\g<cat>/i'])\n",
    ")\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And yeah... I'm showing off with regular expressions, but just wanted to demonstrate that we can used named groups for quite advanced syntax.\n",
    "\n",
    "Having discovered our files, we can then scan these to see if we can find any data for loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from fddc.annex_a.merger import workbook_util\n",
    "\n",
    "worksheets = workbook_util.find_worksheets(files[0])\n",
    "display(f\"We found {len(worksheets)} worksheets.\")\n",
    "\n",
    "asdict(worksheets[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# Matching data to a source\n",
    "\n",
    "Once we have discovered our data, we can try to match it to our source definitions. This first matches the worksheets to a Data Source item based on the worksheet name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fddc.annex_a.merger import matcher\n",
    "\n",
    "matched_sheets, unmatched_sheets = matcher.match_data_sources(worksheets, data_sources)\n",
    "\n",
    "display(f\"We found {len(matched_sheets)} matched sheets.\")\n",
    "\n",
    "asdict(matched_sheets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A 'MatchedSheet' is just a container with a worksheet item and a datasource item. \n",
    "\n",
    "The `match_data_sources` function also returns a list of sheets that were discovered but if not matched:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display(f\"We found {len(unmatched_sheets)} unmatched sheets.\")\n",
    "\n",
    "asdict(unmatched_sheets[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match columns\n",
    "\n",
    "The next step is to try to match the columns in the two datasets, again using the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "sheet_with_headers = matcher.match_columns(matched_sheets)\n",
    "asdict(sheet_with_headers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Reviewing the matches\n",
    "\n",
    "We now have a matched dataset - however, the automatic matching may not have been perfect, we can write a report showing what was matched and what was not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from fddc.annex_a.merger import matcher_report\n",
    "\n",
    "matcher_report.column_report(\n",
    "    sheet_with_headers, \n",
    "    unmatched_sheets, \n",
    "    \"my-match-report.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Improving the matches\n",
    "\n",
    "How about that? It actually returns a Pandas Dataframe.\n",
    "\n",
    "The Excel report can be edited to fix any matches or add files that may have been missed. If you manually add files or delete all the `column_name` values for an entire table, then that table will be rescanned.\n",
    "\n",
    "Let's try a few, just to see what happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rows = [\n",
    "    {\n",
    "        # This will be completely scanned for matches\n",
    "        \"filename\": \"../examples/example-A-2005.xls\",\n",
    "        \"sort_key\": \"2005-A\",\n",
    "    },\n",
    "    {\n",
    "        # This table should be auto-matched\n",
    "        \"filename\": \"../examples/example-B-2004.xlsx\",\n",
    "        \"sort_key\": \"2004-B\",\n",
    "        \"sheetname\": \"List_1\",\n",
    "        \"table\": \"List 1\"\n",
    "    },\n",
    "    {\n",
    "        # The next records will manually map a few columns - I'm purposfully going to change the mappings\n",
    "        # so we can see that it is being applied\n",
    "        \"filename\": \"../examples/example-B-2004.xlsx\",\n",
    "        \"sort_key\": \"2004-B\",\n",
    "        \"sheetname\": \"List_2\",\n",
    "        \"table\": \"List 2\",\n",
    "        \"column_name\": \"Child Unique ID\",\n",
    "        \"header_name\": \"Child Unique ID\"\n",
    "    },\n",
    "    {\n",
    "        \"filename\": \"../examples/example-B-2004.xlsx\",\n",
    "        \"sort_key\": \"2004-B\",\n",
    "        \"sheetname\": \"List_2\",\n",
    "        \"table\": \"List 2\",\n",
    "        \"column_name\": \"Gender\",\n",
    "        \"header_name\": \"Ethnicity\"\n",
    "    },\n",
    "    {\n",
    "        \"filename\": \"../examples/example-B-2004.xlsx\",\n",
    "        \"sort_key\": \"2004-B\",\n",
    "        \"sheetname\": \"List_2\",\n",
    "        \"table\": \"List 2\",\n",
    "        \"column_name\": \"Ethnicity\",\n",
    "        \"header_name\": \"Gender\"\n",
    "    }\n",
    "]\n",
    "report = pd.DataFrame(rows)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "sheet_with_headers, unmatched_sheets = matcher_report.process_report(report, data_sources=data_sources)\n",
    "display(f\"We configured {len(sheet_with_headers)} sheets\")\n",
    "df = matcher_report.column_report(\n",
    "    sheet_with_headers, \n",
    "    unmatched_sheets\n",
    ")\n",
    "df[[\"filename\",\"sort_key\", \"sheetname\",\"table\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that in the second file that we manually matched, has been included. List 1 has been automatically matched since we did not include any values ourselves. However, List 2 has only matched the columns we provided, including the wrong matches. The unmatched values are included in the report so they can be matched later if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.filename==\"../examples/example-B-2004.xlsx\"][[\"table\",\"column_name\",\"header_name\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging data\n",
    "\n",
    "Now that we have our merge configuration, we're ready to merge the contents of the different tables. \n",
    "\n",
    "The merge process is relatively simple. The data is read from each source, then normalised so that all the different sources are comparable. The data is sorted by the 'sort_key' from the configuration, and then deduplicated based on the unique columns in the configuration. When multiple records are found, the latest record is kept based on the sort_key order.\n",
    "\n",
    "Finally, as we need columns to be comparable to deduplicate, we can also force some columns to conform to be a date, for example. This is also set in the configuration. \n",
    "\n",
    "Let's manually merge our List 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter List 2 from our previous results\n",
    "list_2 = [s for s in sheet_with_headers if s.sheet.source_config.name == \"List 2\"]\n",
    "data_source_config = next(iter([ds for ds in data_sources if ds.name == \"List 2\"]))\n",
    "\n",
    "display(len(list_2))\n",
    "data_source_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fddc.datatables import load, normalise, merge\n",
    "\n",
    "\n",
    "df_1 = load.load_dataframe(list_2[0].sheet.sheet_detail)\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = load.load_dataframe(list_2[1].sheet.sheet_detail)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now need to make sure that both dataframes have the same columns \n",
    "\n",
    "Ok, so that's not very exciting for our example - but since we're remapping some columns we'll get an idea of how it works.\n",
    "\n",
    "One of the advantages of our configuration objects is that they generate a column name map between what is in the source data and what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The first one isn't very exciting because it automatically matched all of the columns\\n\")\n",
    "display(list_2[0].column_map())\n",
    "\n",
    "print(\"\\n\\nThe second shows clearly how we remapped some of the columns\\n\")\n",
    "display(list_2[1].column_map())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_normalised = normalise.normalise_dataframe(df_1, data_source_config.column_names(), list_2[0].column_map())\n",
    "df_1_normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2_normalised = normalise.normalise_dataframe(df_2, data_source_config.column_names(), list_2[1].column_map())\n",
    "df_2_normalised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how gender and ethnicity is swapped, and all the other columns are blank as we did not map those. There is an option to retain those that already have the correct names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2_normalised = normalise.normalise_dataframe(df_2, data_source_config.column_names(), list_2[1].column_map(), \n",
    "                                     only_retain_mapped=False)\n",
    "df_2_normalised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean columns\n",
    "\n",
    "We can see that somehow the date for 1008 has been corrupted and it's upset the entire column - it would also stop us from merging these if we had a unique flag on that column as we couldn't accurately compare the values. We therefore use the settings from our configuration to make sure both tables have the correct types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_cleaned = normalise.clean_datatypes(df_1_normalised, data_source_config.columns)\n",
    "df_2_cleaned = normalise.clean_datatypes(df_2_normalised, data_source_config.columns)\n",
    "df_2_cleaned[df_2_cleaned[\"Child Unique ID\"].isin({1007,1008,1009})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the removed erroneous date, and the intentional ID error there as well (actually - I messed up)? Hopefully the merge should get rid of the duplicates ðŸ¤ž"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join them up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.merge_dataframes([df_1_cleaned, df_2_cleaned], data_source_config.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's great! But... it has used the slightly older erroneous data where instead of the newer one. That's because we didn't use our sort key!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_cleaned['sort_key'] = list_2[0].sheet.sheet_detail.sort_key\n",
    "df_2_cleaned['sort_key'] = list_2[1].sheet.sheet_detail.sort_key\n",
    "\n",
    "merge.merge_dataframes([df_1_cleaned, df_2_cleaned], data_source_config.columns, sort_key='sort_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
